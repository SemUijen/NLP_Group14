{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d31155-d02a-4249-8fe7-71e34963c7c5",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227d8e2f-ebec-4fb4-a8a9-3eff2888211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d16c8b3-65f6-4f10-9901-e4d1d5a4daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06aa59d2-d31d-4431-8e71-e08eb8dc31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"../data/aapl_us_equities_news_proc_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee6c5ce-8de5-4144-9683-f41ad80e761b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a9c9d-e119-458a-8e50-326ea48f611f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Consolidate text\n",
    "\n",
    "Concats every article and title together per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0aac7c-f819-40d6-9069-fbe27d7e31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(series):\n",
    "    return reduce(lambda x, y: x + \" \" + y, series)\n",
    "\n",
    "\n",
    "# Group by date\n",
    "df_data = df_data.groupby(\"date\").agg({\"title\": concat, \"content\": concat, \"target\": \"first\"})\n",
    "\n",
    "# Concat title and content\n",
    "df_data[\"text\"] = df_data[\"title\"] + \" \" + df_data[\"content\"]\n",
    "\n",
    "# Remove index\n",
    "df_data = df_data.reset_index()\n",
    "\n",
    "# Select columns\n",
    "df_data = df_data[[\"text\", \"target\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad20348-cb76-44a3-9122-2fdba168b75a",
   "metadata": {},
   "source": [
    "## 1.2 Remove HTML tags\n",
    "\n",
    "Removes HTML tags using `beautifulsoup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21cda14-523d-4280-8a42-afd2e77b2c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1740/1740 [00:00<00:00, 4015.00it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Remove html tags from text.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "df_data[\"text\"] = df_data.progress_apply(lambda row : remove_html_tags(row[\"text\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d1ce0-08b3-49ce-8afa-1b9f40156654",
   "metadata": {},
   "source": [
    "## 1.3 Remove EOL characters\n",
    "\n",
    "Removes characters like: `\\n`, `\\t`, `\\r`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b95c62-097f-4961-9d4b-f0b7b1366a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1740/1740 [00:00<00:00, 5035.19it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_eol_characters(text):\n",
    "    \"\"\"\n",
    "    Remove EOL characters from text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"(\\n|\\t|\\r)\", \"\", text)\n",
    "\n",
    "\n",
    "df_data[\"text\"] = df_data.progress_apply(lambda row: remove_eol_characters(row[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264bb3e0-a7ca-4db3-b2d4-3118c41fb442",
   "metadata": {},
   "source": [
    "## 1.4 Remove other characters\n",
    "\n",
    "Removes all characters except: `a-z`, `A-Z`, `À-ȕ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913361e9-a214-44d1-b331-bd303652cfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1740/1740 [00:02<00:00, 797.03it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_other_characters(text):\n",
    "    \"\"\"\n",
    "    Remove other characters from text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"[^a-zA-ZÀ-ȕ]\", \" \", text)\n",
    "\n",
    "\n",
    "df_data[\"text\"] = df_data.progress_apply(lambda row: remove_other_characters(row[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a249792-b1e2-4598-beb5-7faf11a128b5",
   "metadata": {},
   "source": [
    "## 1.5 Remove excessive spaces\n",
    "\n",
    "Removes excessive spaces in some documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "300f0f7d-9622-4f56-b3a9-6d4f4ef72d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1740/1740 [00:01<00:00, 1424.59it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_excessive_spaces(text):\n",
    "    \"\"\"\n",
    "    Remove excessive spaces from text.\n",
    "    \"\"\"\n",
    "    return re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "\n",
    "df_data[\"text\"] = df_data.progress_apply(lambda row: remove_excessive_spaces(row[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2facfe-129f-4088-8079-c2c6c6d95203",
   "metadata": {},
   "source": [
    "## 1.6 Convert to lowercase\n",
    "\n",
    "Converts all characters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09f03f2c-a82c-41d8-a04b-5c4a52d02fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1740/1740 [00:00<00:00, 33429.63it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    \"\"\"\n",
    "    Convert to lowercase.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "df_data[\"text\"] = df_data.progress_apply(lambda row: convert_to_lowercase(row[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d65ba-843f-4b75-8b9a-2f0853b3ab8e",
   "metadata": {},
   "source": [
    "## 1.7 Remove non-dictionary words, single-character words, stopwords and lemmatize\n",
    "\n",
    "Removes words that do not occur in the english dictionary, words that consist of one character, words that are a stopword and lemmatizes the remaining words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b0da38-35a7-45ef-8578-296c5b3df0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1740/1740 [13:58<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "dictionary = set(words.words())\n",
    "\n",
    "\n",
    "def remove_non_dictionary_single_character_stopwords_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Remove non-dictionary words, single character words, stopwords and lemmatize\n",
    "    text.\n",
    "    \"\"\"\n",
    "    return \" \".join(token.lemma_ for token in nlp(text) if len(token.text) > 1 and not token.is_stop and token.text in dictionary)\n",
    "\n",
    "\n",
    "df_data[\"text\"] = df_data.progress_apply(lambda row: remove_non_dictionary_single_character_stopwords_lemmatize(row[\"text\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6253cd4f-34f1-4cc2-b95b-47a3e61426d2",
   "metadata": {},
   "source": [
    "## 1.8 Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6731de23-525c-46e3-a15b-7d478913f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(???): enkele HTML tags (div, img, margin, etc.)\n",
    "# TODO(???): Verwijder woorden die er maar een keer inzitten\n",
    "# TODO(???): Verwijder woorden die veel te veel voorkomen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
